---
title: "metabolomics_analysis"
author: "Maureen Carey"
date: "4/5/2021"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = '~/Documents/work/TUMI/microbiome_course_2021/')
knitr::opts_knit$set(root.dir = '/Users/maureencarey/local_documents/work/TUMI_efforts/microbiome_course_2021/')
```

# Summary of experiment (from the scientists)^[Data were provided by Nishikant Wase and preprocessed on the previous day of this workshop. Data are unpublished.]

Maize seeds were inoculated with either water or *Pseudomonas fluorescens* in three separate experiments and allowed to germinate.  Seeds treated with the bacteria showed a difference in both root and shoot growth. The root tissues were extracted by grinding with liquid nitrogen and stored for future use in untargeted metabolomics experiment.  

Our main aim for this experiment was to understand metabolomic changes in roots treated either with water or with suspension of *P.fluorescens*.

In this experiment, we ran 5 replicates of the mock treatment and 4 replicates of PF treated roots. A pooled quality control sample was created after the extraction by aliquoting 10 uL of metabolite extract from each sample for identification and quality control. Blank samples are 0.1% Formic acid in water. 

# Prep computational environment

```{r}

library(tidyverse)
library(randomForest)
library(pROC)
library(readxl)
library(ggpubr)
library(vegan)
library(DESeq2)

# experiment metadata
metadata = read_excel("./metabolomics/processed_metabolomics_data.xlsx", sheet = "sampleMetadata")
# mass spec results (Note: Nishikant Wase's previous day of workshops generated this file from the raw spectra)
df = read_excel("./metabolomics/processed_metabolomics_data.xlsx", sheet = "FilteredDataMatrix")

```

# Start data preprocessing

Look at metabolomics data for each sample group.

```{r}

# looks at column names in data
colnames(df)
# separate metabolite information from metabolite abundances
met_abund = df[,colnames(df) %in% c("Alignment ID",
                                    "x01222021x_blank1","x01222021x_blank2",
                                    "x01222021x_Met_Mock_R1","x01222021x_Met_Mock_R2",
                                    "x01222021x_Met_Mock_R3","x01222021x_Met_Mock_R4",
                                    "x01222021x_Met_Mock_R5",
                                    "x01222021x_Met_PF_R2","x01222021x_Met_PF_R3",
                                    "x01222021x_Met_PF_R4","x01222021x_Met_PF_R6",
                                    "x01222021x_Met_pooled_QC1","x01222021x_Met_pooled_QC2",
                                    "x01222021x_Met_pooled_QC3","x01222021x_Met_pooled_QC4")]
met_info = df[,!(colnames(df) %in% c("x01222021x_blank1","x01222021x_blank2",
                                     "x01222021x_Met_Mock_R1","x01222021x_Met_Mock_R2",
                                     "x01222021x_Met_Mock_R3","x01222021x_Met_Mock_R4",
                                     "x01222021x_Met_Mock_R5",
                                     "x01222021x_Met_PF_R2","x01222021x_Met_PF_R3",
                                     "x01222021x_Met_PF_R4","x01222021x_Met_PF_R6",
                                     "x01222021x_Met_pooled_QC1","x01222021x_Met_pooled_QC2",
                                     "x01222021x_Met_pooled_QC3","x01222021x_Met_pooled_QC4"))]
# identify sample groups
grps = c("blank","blank",
         "Mock","Mock","Mock","Mock","Mock",
         "PF","PF","PF","PF",
         "pooledQC","pooledQC","pooledQC","pooledQC")
names(grps) = c("x01222021x_blank1","x01222021x_blank2",
                "x01222021x_Met_Mock_R1","x01222021x_Met_Mock_R2",
                "x01222021x_Met_Mock_R3","x01222021x_Met_Mock_R4",
                "x01222021x_Met_Mock_R5",
                "x01222021x_Met_PF_R2","x01222021x_Met_PF_R3",
                "x01222021x_Met_PF_R4","x01222021x_Met_PF_R6",
                "x01222021x_Met_pooled_QC1","x01222021x_Met_pooled_QC2",
                "x01222021x_Met_pooled_QC3","x01222021x_Met_pooled_QC4")

```

Double check missing values have been removed/imputed.

```{r}

min(met_abund[,-1])>0

```

What would happen if there were 'NA's indicating missing values in the dataset? Try testing this:

```{r, eval = FALSE }

min(c(1,2,NA))

```

Ok, so missing values have been eliminated by removing metabolites that were frequently below the limit of detection and/or imputation. 

------

On to our analyses!

# Plot the data

I always like to take a look at the data before I get started. This will clue you in to any potential challenges that may arise or any issues you need to correct (i.e. do you have sufficient N to detect a difference in sample groups? is there an outlier that you want to follow up on?)

## Basic plotting

For example, let's say you knew Alignment ID 969 was interesting based on previous experiments. What metabolite does Alignment ID 969 represent?

```{r}

interesting_met = met_info[met_info$`Alignment ID` == 969,] %>% pull(`Metabolite name`)

interesting_met

```

Now let's see the relative abundance of `r interesting_met` in each group.

```{r}

# extract metabolite 969 from dataset
interesting_met_abund = met_abund[met_abund$`Alignment ID` == 969,]
# remove column with Alignment ID, since we only have one left in our dataset
interesting_met_abund$`Alignment ID` = NULL
# merge metabolite abundance with group names
interesting_met_abund_w_grps = rbind(interesting_met_abund, grps)
# flip dataframe for easier plotting
df_to_plot = as.data.frame(t(interesting_met_abund_w_grps))
# add column headers for easier data access
colnames(df_to_plot) = c('metabolite_abundance', 'group')
# make sure the abundances are read as numbers not as text
df_to_plot$metabolite_abundance = as.numeric(df_to_plot$metabolite_abundance)

# plot
ggplot(data = df_to_plot, aes(x = group, y = metabolite_abundance, color = group)) + # set variables
  geom_boxplot() + # make it a boxplot
  geom_point(size = 4) + # add points
  guides(color = F) # hide legend
  
```

By eye, it looks like there may not be a difference in `r interesting_met` between the infected (PF) group and the mock treated group, especially since this metabolite can be detected at similar levels in the blank sample. However, performing a statistical test would give us higher confidence in this conclusion. Here's one way we can do this:

```{r}

group_options = unique(df_to_plot$group)
my_comparisons <- list(c(group_options[1], group_options[2]), 
                       c(group_options[1], group_options[3]),
                       c(group_options[2], group_options[3]))

# remake previous figure
p = ggplot(data = df_to_plot, aes(x = group, y = metabolite_abundance, color = group)) + 
  geom_boxplot() + 
  geom_point(size = 4) +
  guides(color = F)
# add statistics 
p + stat_compare_means(comparisons = my_comparisons, size = 3, method = 't.test')

```

A t-test confirms that there is no meaningful difference in `r interesting_met` in this study.

FYI, the function we just used to perform a t.test ('stat_compare_means') does not do multiple testing corrections. We will discuss that later!

------

WERE ANY NORMALIZATION STEPS PERFORMED? CHECK WITH NISHI?

Next, let's see if yesterday's normalization efforts worked. We'd expect to see a similar range of metabolite abundances in all sample groups. A histogram would be useful for this purpose.

```{r}

# remove Alignment ID column
met_abund_temp = met_abund
met_abund_temp$`Alignment ID` = NULL
# merge metabolite abundance with group names
met_abund_w_grps = rbind(grps,met_abund_temp)
# flip dataframe for easier plotting
df_to_plot = as.data.frame(t(met_abund_w_grps))
# add column headers for easier data access
colnames(df_to_plot)[1] = 'group'
# reorganize data
df_to_plot = tidyr::pivot_longer(df_to_plot, !group, names_to = "metabolite", values_to = "abundance")
# make sure the abundances are read as numbers not as text
df_to_plot$abundance = as.numeric(df_to_plot$abundance)

ggplot(data = df_to_plot, aes(x = abundance, fill = group)) +
  geom_histogram(bins = 100) + scale_x_sqrt() +
  facet_wrap(~group) + guides(fill = F)

```

PS - Have questions about that 'pivot_longer' step? Check this out:^[from https://fromthebottomoftheheap.net/2019/10/25/pivoting-tidily/]

![this is the concept behind pivoting - a way to restructure data for (sometimes) easier plotting](/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/metabolomics/tidyr-longer-wider.gif)

## Principal component analysis

One of my favorite tools to use before digging into specific analysis is principal component analysis. High dimensional data is often defined as datasets that have more variables than observations (e.g. more metabolites than samples). It is challenging to use many simple statistical methods with these datasets, so dimensionality reduction is a useful tool. I recommend you read [this paper](https://www.nature.com/articles/nrm2041) to better understand dimensionality reduction.

Let's define a couple functions that will make our PCA plot easier.

```{r}

get_1st = function(input_pca) {
  # class(input_pca) = "prcomp"
  temp = summary(input_pca)
  temp2 = temp$importance
  return(round(temp2[2,1]*100, digits = 2))
}

get_2nd = function(input_pca) {
  temp = summary(input_pca)
  temp2 = temp$importance
  return(round(temp2[2,2]*100, digits = 2))
}

median_center_scale_data <- function(x) {
  # x is matrix or dataframe with rows as samples and columns as mets
  sd_x = apply(x,2,sd,na.rm=TRUE)
  x_centered = apply(x, 2, function(y) (y - median(y,na.rm=TRUE)))
  x_centered_scaled = sweep(x_centered, 2, sd_x, "/")
  indx = setdiff((which(is.na(x_centered_scaled))),which(is.na(x_centered)))
  if (any(is.na(x_centered_scaled))) { 
    x_centered_scaled[indx] = 0}
  if (any(is.na(x_centered_scaled))) { warning('sd == 0')}
  return(x_centered_scaled)}

```

Before performing PCA, the data needs to be normalized. Specifically, we need to perform 'feature' normalization by logging values and then centering and scaling them on a feature-by-feature basis (or, metabolite-by-metabolite basis).

```{r}

# remove Alignment ID column
met_abund_temp = met_abund
met_abund_temp$`Alignment ID` = NULL

# Feature normalization (log, center, scale)
# log
log_data = log2(met_abund_temp)
# prep format for PCA
data_transposed = t(log_data)
# [median] center & [sd] scale
data_for_pca = median_center_scale_data(data_transposed) 

```

We can confirm these steps by again plotting histograms of the dataset.

```{r}


# merge metabolite abundance with group names
met_abund_w_grps = cbind(grps,as.data.frame(data_for_pca))
# reorganize data
df_to_plot = tidyr::pivot_longer(met_abund_w_grps, !grps, names_to = "metabolite", values_to = "abundance")

ggplot(data = df_to_plot, aes(x = abundance, fill = grps)) +
  geom_histogram(bins = 100) + 
  facet_wrap(~grps) + guides(fill = F)

```

With the normalized data, we can perform PCA.

```{r}

pca_data = prcomp(data_for_pca, center = F, scale. = F)
summary(pca_data)
# get first two components
data_plot = data.frame(pca_data$x[,c(1,2)]) 

```

And plot it: 

```{r}

group = factor(metadata$group)
ggplot(data = data_plot, aes(x = PC1, y = PC2))+
  geom_point(aes(color = group), size=I(2), alpha = .9) +#, show.legend = FALSE) +
  theme(axis.text = element_text(family = "Helvetica")) + 
  ylab(paste("PC2 (",paste(get_2nd(pca_data),"%)", sep = ""),sep = "")) + 
  xlab(paste("PC1 (",paste(get_1st(pca_data),"%)", sep = ""),sep = "")) 


```

You may also want to perform a statistical test, a PerMANOVA, to see if the clusters separate.

```{r}

t = adonis(data_plot ~ group, data = metadata, method = "eu") # PerMANOVA
print(t)

```

Note: ever wonder what the difference between PCA and PCoA is? PCoA, or Principal coordinate analysis, is a 'fancy' PCA, prinipal component analysis. If you were to calculate the distance between all samples, basically tabulate the physical distance between all points in a PCA plot, and then perform PCA on those distance calculations, you would have performed a PCoA. This is particularly useful when you want to use more complex distance calculations (i.e. phylogenetic relatedness) or use multiple variable types (i.e. metabolomics data AND microbiome data).

# Removing our QC samples

We are not actually interested in the metabolites detected in the blank or QC (quality control) samples. We need to remove these. However, as shown with `r interesting_met`, we don't want to consider the metabolites that were measured in the blank samples. 

Let's first find the mean of all metabolites detected in the blanks samples. Then we will subtract these abundances from all measured samples.

```{r}

# get only the values that are associated with blank samples
blanks = as.data.frame(met_abund[,colnames(met_abund) %in% c("Alignment ID","x01222021x_blank1","x01222021x_blank2")])
# make metabolite ID into the rownames, rather than a column
rownames(blanks) = blanks$`Alignment ID`
# remove metabolite ID column
blanks$`Alignment ID` = NULL
# get mean value for each metabolite
blanks = base::rowMeans(blanks)

```

Now we can substract the mean blank value from the abundance of each sample.

```{r}

# make metabolite ID into the rownames, rather than a column
met_abund_temp = met_abund
rownames(met_abund_temp) = met_abund_temp$`Alignment ID`
# remove metabolite ID column
met_abund_temp$`Alignment ID` = NULL
# drop the blank columns
met_abund_temp = met_abund_temp[,!(colnames(met_abund_temp) %in% c("x01222021x_blank1","x01222021x_blank2"))]
# add mean of blank values
met_abund_temp = cbind(met_abund_temp,blanks)
met_abund_normalized_to_blanks = met_abund_temp[1:ncol(met_abund_temp)-1]-met_abund_temp[,ncol(met_abund_temp)]
# drop pooled columns
met_abund_normalized_to_blanks = met_abund_normalized_to_blanks[,!(colnames(met_abund_normalized_to_blanks) %in% c("x01222021x_Met_pooled_QC1", "x01222021x_Met_pooled_QC2", "x01222021x_Met_pooled_QC3","x01222021x_Met_pooled_QC4"))]
# replace negative values with a very small number
# replacing with 0 will generate errors at the log2 step below
min_Val = min(met_abund_normalized_to_blanks[met_abund_normalized_to_blanks > 0])
met_abund_normalized_to_blanks[met_abund_normalized_to_blanks <= 0] = min_Val/10000

```

Let's redo the PCA in light of these changes.

```{r}

# remove Alignment ID column
met_abund_temp = met_abund_normalized_to_blanks
met_abund_temp$`Alignment ID` = NULL

# Feature normalization (log, center, scale)
# log
log_data = log2(met_abund_temp)
# prep format for PCA
data_transposed = t(log_data)
# [median] center & [sd] scale
data_for_pca = median_center_scale_data(data_transposed) 

# do PCA
pca_data = prcomp(data_for_pca, center = F, scale. = F)
summary(pca_data)
# get first two components
data_plot = data.frame(pca_data$x[,c(1,2)]) 

# plot
group = gsub('.{3}$', '', rownames(data_for_pca))
group = factor(gsub('x01222021x_Met_', '', group))
ggplot(data = data_plot, aes(x = PC1, y = PC2))+
  geom_point(aes(color = group), size=I(2), alpha = .9) +#, show.legend = FALSE) +
  theme(axis.text = element_text(family = "Helvetica")) + 
  ylab(paste("PC2 (",paste(get_2nd(pca_data),"%)", sep = ""),sep = "")) + 
  xlab(paste("PC1 (",paste(get_1st(pca_data),"%)", sep = ""),sep = "")) 

```

This dataset without the blanks and QC samples will be used from here on out.

# More multivariate statistics

Multivariate statistics come in several flavors - unsupervised and supervised. Unsupervised analyses do not require the input of any grouping data, whereas supervised analyses aim to distinguish members of groups (e.g. plants infected with PF from mock-infected plants). PCA is an unsupervised analysis whereas random forest classification and partial least squares discriminant analysis are supervised techniques.

```{r}


```

## Perform machine learning with Random Forest Classification

Random forest DESCRIPTION

If you have sufficient N, it is wise to split the dataset into training and test sets. Even though random forest is an internally validated (i.e. it performs a training and test split for every tree), splitting your data into a training and test dataset increases your confidence in the quality of the model and minimizes overfitting. This can only be performed if you have sufficient N and would look something like this:

```{r, eval = F}

# Set random seed (otherwise random splits will be irreproducibly random!)
set.seed(2021)
# transpose to have rows as samples and columns as mets
t_met_abund_normalized_to_blanks = t(met_abund_normalized_to_blanks)
# Generate a random sample with half of the dataset
indexes = sample(1:nrow(t_met_abund_normalized_to_blanks), size = floor(nrow(t_met_abund_normalized_to_blanks)/2))
# Split into training and test sets
training = t_met_abund_normalized_to_blanks[indexes,]
test = t_met_abund_normalized_to_blanks[-indexes,]

```

Since this project has only 9 samples, we cannot/should not do this. So we will use the full dataset.

First, we will build the random forest classifier.

```{r}

# Set random seed (otherwise random splits will be irreproducibly random!)
set.seed(2021)
# transpose to have rows as samples and columns as mets
data_for_rf = as.data.frame(t(met_abund_normalized_to_blanks))
# add group to dataset
group = gsub('.{3}$', '', rownames(data_for_rf))
group = factor(gsub('x01222021x_Met_', '', group))
data_for_rf$group = group
# modify how metabolite IDs are referenced because randomForest doesn't handle column names as numbers well
colnames(data_for_rf) = paste('met_',colnames(data_for_rf), sep = '')
colnames(data_for_rf)[colnames(data_for_rf) == 'met_group'] = 'group'
rf_classifier = randomForest(group ~ ., data=data_for_rf, ntree=100, mtry=2, importance=TRUE)

```

Look at summary of model performance

```{r}

rf_classifier

```

Ok, so the model is not great. We are seeing a high out-of-bag (OOB) error rate and the confusion matrix indicates that we did not often accurately predict sample group.

Let's take a deeper look.

Look at model performance on the testing dataset. How well does the model correctly label samples?

```{r}

predictions = as.data.frame(predict(rf_classifier,type="prob"))
predictions$predict = names(predictions)[1:2][apply(predictions[,1:2], 1, which.max)]
predictions$observed = data_for_rf$group

```

A ROC curve, or receiver operating characteristic curve, is a helpful tool to evaluate a random forest classifier and graphically shows the predictive ability of a classifier over a range of parameters. This is only useful for binary classifiers.

There is always a tradeoff between sensitivity and specificity, but at NOT DONE

Just a reminder, sensitivity is the true positive rate:

$$
\frac{TruePositives}{TruePositives + FalseNegatives}
$$

Specificity is the true negative rate:

$$
\frac{TrueNegatives}{TrueNegatives + FalsePositives}
$$

```{r}

# prep receiver operator curve (ROC)
ROC_val = roc(ifelse(predictions$observed=='PF', 'PF', 'Mock'), as.numeric(predictions$PF))

  
```

AUC - NOT DONE

```{r}

AUC_val = auc(ROC_val)
  
```


ROC curve

```{r}

ggroc(list(ROC_val), aes = "colour") +
  theme_bw() +
  geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color="grey", linetype="dashed") +
  scale_color_manual(values = c("blue","red"), guide = F) +
  theme(plot.margin=unit(c(.5,.5,.5,.5),"cm"),
        legend.title = element_blank()) +
  annotate("text", label = paste0("AUC = ",round(AUC_val, digits = 2)), x = .25, y = .27, color = "blue") 

# use ggsave() to save figure!

```

This model does not perform well, so we should not over interpret the results of this classifier. However, I want to walk you through some interpretive analyses. If the model was predictive, we might be interested in identifying which variables are important. 

*MeanDecreaseAccuracy* (or the mean decrease in accuracy) is a metric of variable important and tells you the effect of removing a variable on model performance. *MeanDecreaseGini* (the mean decrease in Gini index) is another (more complex) metric of variable importance and quantifies the measure of 'node impurity.' This evaluates how well a variable splits your classification groups.

```{r}

varImpPlot(rf_classifier)

```

Cool, but what are the names of these metabolites? We will have to do some data wrangling to make this more interpretable.

```{r}

# these are the metabolite names for the Alignment IDs - keep this
met_names = met_info[,c("Alignment ID", "Metabolite name")]
# but remember we added 'met_' to the beginning of each alignment ID for the random forest? well, we did, so let's do that here
met_names$`Alignment ID` = paste('met_',met_names$`Alignment ID`,sep = '')

# let's extract the node impurity score from our classifier
extracted_importance = importance(rf_classifier,type=2) # type 2 = node impurity
# sort node impurity scores
extracted_importance = extracted_importance[rownames(extracted_importance)[order(extracted_importance)],]
# extract most important variables
top10 = tail(extracted_importance,10)
# reorganize these important variables into dataframe
top10 = as.data.frame(top10) %>% rownames_to_column('variable') %>% dplyr::rename(value = top10)
# get the full names of only the top 10 most important variables
met_names = met_names[met_names$`Alignment ID` %in% top10$variable,]
# merge variable importance with met name
plot_df = merge(met_names,top10, by.x= 'Alignment ID', by.y = "variable", all.x = F, all.y = T)

```

Ok now we can plot it.

```{r}

ggplot(data = plot_df) +
  geom_bar(aes(x=reorder(`Metabolite name`,value), y = value), 
           stat = "identity", width=0.4) +
  xlab(NULL) + 
  ylab('Mean decrease in node impurity') + 
  coord_flip()
#ggsave("/path/to/folder.png",width = 7,height = 3,units = "in",dpi = 300)

```



# Perform univariate statistics - CUT

## Prep the dataset

```{r}

# transpose and replace zeros
met_count = met_abund_normalized_to_blanks
met_count[met_count == 0] = min(met_count)/10000

# subset metadata for only Mock infected and PF infected samples 
meta_use = metadata[metadata$rawFileName %in% colnames(met_count),]
# make DESeq obj
dds_PD = DESeqDataSetFromMatrix(countData = as.data.frame(met_count),
                              colData = meta_use,
                              design= ~group)
dds_PD = DESeq(dds_PD)
# shrinkage
# because we are interested in treated vs untreated, we set 'coef=2'
resNorm_PD <- lfcShrink(dds_PD, coef=2, type="ashr") # resultsNames(dds_proc)[2]

#resOrdered <- resNorm_PD[order(resNorm_PD$pvalue),]
summary(resNorm_PD)


# Create the volcano plot - PREP FOR PREDETECTION
resNorm_PD_df = as.data.frame(resNorm_PD) %>% 
  rownames_to_column('ASV') %>%
  mutate(col = ifelse(padj<0.05,1,0),
         lab = ifelse(log2FoldChange>1.5 | abs(log2FoldChange) > 2,1,0),
         sig = -log10(padj))
sum(is.infinite(resNorm_PD_df$sig))
sum(is.na(resNorm_PD_df$sig))
resNorm_PD_df = resNorm_PD_df %>% mutate(padj = ifelse(is.na(padj),pvalue,padj),
                                         sig = ifelse(is.na(sig),0,-log10(pvalue)),
                                         col = ifelse(is.na(col),0,col),
                                         lab = ifelse(is.na(lab),0,lab))
#resNorm_PD[is.infinite(resNorm_PD$sig),"sig"] <- 350
sum(is.na(resNorm_PD_df$pvalue))
sum(is.infinite(resNorm_PD_df$pvalue))
#genes.to.plot <- !is.na(resNorm_PD$pvalue)

# Label ASVs
tax = tax_table(ps_rare_diff_temp_PD)
tax = tax[rownames(tax) %in% resNorm_PD_df$ASV,]
tax = merge(tax,resNorm_PD_df, by.x= 'row.names', by.y = "ASV", all.x = F, all.y = T)
tax$Genus = as.character(tax$Genus)
tax2 = tax %>% 
  transform(id=as.numeric(factor(Row.names))) %>%
  mutate(var = ifelse(is.na(Family),
                      as.character(Row.names),
                      ifelse(is.na(Genus),paste0('[f] ',Family),Genus)))

tax2 = merge(merge(tax2, as.data.frame(top10_PD)%>% rownames_to_column('Row.names'), by = "Row.names", all = T),
             as.data.frame(top10_TOD)%>% rownames_to_column('Row.names'), by = "Row.names", all = T)
tax2 = tax2 %>% 
  mutate(col2 = ifelse(col == 0, 'non sig',ifelse(!is.na(top10_TOD) & !is.na(top10_PD),'sig but dif color','sig')),
         size2 = ifelse(!is.na(top10_PD),1,0))


ggplot(predetection_tax2 %>% mutate(timepoint = 'pre-detection')) + 
  geom_point(aes(x = log2FoldChange, y = sig, color = as.factor(col2), size = as.factor(size2) ), alpha = .2)+
  xlab("Effect size: log2(fold change)") + ylab("-log10(adjusted p-value)")+
  scale_color_manual(values = c('black',"red","purple"),  guide = F) +
  scale_size_manual(values = c(1,3),  guide = F) + facet_wrap(~timepoint)
ggsave("/Users/maureencarey/local_documents/work/crypto_microbiome/cryptosporidium_microbiome/figures/Fig4B_univar_PD.png",
       width = 4,height = 4,units = "in",dpi = 300)

```

## Don't forget multiple testing correction!
