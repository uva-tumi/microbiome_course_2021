---
title: "16S rRNA gene sequencing analysis"
author: "Pankaj Kumar, Maureen Carey"
output:
  html_document:
    df_print: paged
---

# Step 0: Know the experiment and get the data!

### The experiment^[Data are unpublished and provided by Carrie Cowardin.]

Carrie Cowardin briefed you on this dataset yesterday. Essentially, gnotobiotic mice were colonized with a synthetic microbiome, a community with 25 members. These mice were then split into two groups and fed two distinct diets (control and supplemented). We are interested in how the fecal microbiome composition may vary as a result of these diets.

DNA was extracted from stool samples and the V4 region of the 16S rRNA gene was amplified with a modified Caporaso et al. protocol. 250 base pairs were then sequenced using paired end sequencing on a MiSeq. To analyses these data, one of the most important pieces of information is what primers were used. As a reminder, the experimental conversion from stool to amplified 16S rRNA gene requires:

1. DNA extraction
2. DNA amplification
3. Multiplexed library preparation 
4. Sequencing

Analysis then requires:

1. De-multiplexing
2. Trimming adapter sequences (added in the sequencing process)
3. Trimming primer sequences (added during the DNA amplification process)
4. Analysis like the one provided below

Most sequencing service providers will demultiplex the data for you - essentially, multiple samples from one or more experiments can be run on one sequencing run to save cost, but you may want to analyze the sequences from each sample independently. Multiplexing is a process of adding sequence barcodes so that you can disentangle the sequences from one run into those associated with individual samples. Demultiplexing is the process of interpreting these barcodes to separate sample-by-sample data. If you received files with the sample name in the file name, the data has been demultiplexed.

The microbiome analyst (you!) will be responsible for trimming adapter sequences. These adapters are quite standard and depend on the library preparation kit you've used (i.e. Illumina's Nextera® Library Prep for the MiSeq® System). The script we have included for trimming primers and adapters uses a generic fasta file with over 150 different commonly used adapter options.

The analyst will also be responsible for trimming primer sequences. These are less standardized - common examples are from Faith et al 2013 or Caporaso et al 2011 - and can be technically more challenging to remove because of variable bases and/or variable linker sequences (i.e. a mix of 4 primers may be used with conserved beginning and ends but variable middle sections). **It is essential for the accuracy of your results to trim the correct primer sequences.** If you are the computational analyst and your experimental collaborators do not provide the primer sequences, you cannot proceed. The script we have included for trimming primers and adapters shows how we removed the Caporaso et al primers and a general framework for how to remove primer sequences.

FYI these steps (demultiplexing and adapter and primer trimming) require some additional software. We will not ask you to perform these steps today, but have provided code for your reference to do so!

### The data

Sequencing data will be shared in a specialized text file with the extension `.fastq`. These files contain both the sequences and quality information about each base pair in the sequence. For this study, those data will be provided at the beginning of the workshop in the course Google Drive. The data is unpublished so please do not share without Carrie Cowardin's permission.

### FYI

The tutorials for [dada2](https://benjjneb.github.io/dada2/tutorial.html) and [DESeq2](http://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html) are great extra resources for you (and inspiration for this tutorial)!

# Step 1: Install necessary packages

See installation_instructions file provided in the course Google Drive or on Github in the [pre-course material folder](https://github.com/uva-tumi/microbiome_course_2021).

One minor update, please also install the following packages:

```{r, eval = F}

BiocManager::install(version = '3.12')
BiocManager::install(c("DESeq2","apeglm","ggrepel"))

```

# Step 2: Load the computational environment

Note: you will need to change your root directory. This is where you installed the course folder (`microbiome_course_2021`).

If you are working with a Mac it will look like `/Users/yourusername/Documents/microbiome_course_2021`. If you are working on Windows, it will look like `C:\Users\yourusername\Documents\microbiome_course_2021`. Please note that your path is referenced throughout this document. If it is not changed, you will run into errors!

```{r,warning=TRUE, message=FALSE}

library("dada2")
library("tictoc")
library("DESeq2")
library("phyloseq")
library("Biostrings")
library("ggplot2")
library("ape")
library("ggrepel")

path = "/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021"
path_results = "/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S"
path_trimmed = "/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/trimmed_for_students"
path_reference = "/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/reference"

knitr::opts_knit$set(root.dir = path)

```

Tictoc is a package that provides timing functions. Basically, if you run `tic()` BEFORE and `toc()` AFTER a set of commands, then you will see how long those commands took to run. Sequence analyses are generally quite slow, so we've included these functions for you to evaluate the speed of your computer. If you are struggling with slow running steps, just ask and we can share the results of intermediate steps with you! Running tic/toc is not necessary, but can be helpful!

Also, the more reads you have, the slower these steps will run. So, if you have more samples than 19 or if the read depth is higher than we have for this study, this code will take longer to run. Moving your analysis from a laptop to a desktop or to a server like UVa High Performance Computing's Rivanna can make the analysis much faster.

# Step 3: Set directory path containing the fastq files

These fastq files were generated by 2x250 Illumina Miseq amplicon sequencing of the V4 region of the 16S rRNA gene (for more information about the data refer to the Tuesday lecture or check in with Carrie Cowardin). 

```{r}
tic()
list.files(path_trimmed)
toc()
```

# Step 4: Read the names of the fastq files.

Perform some string manipulation to get matched lists of the forward and reverse fastq files since this is paired end sequencing.

Forward and reverse fastq filenames have format: `SAMPLENAME_1_PT2.fq.gz` and `SAMPLENAME_2_PT2.fq.gz`. "PT2" is my shorthand to indicate that a second round of primer trimming has been performed. "fq" is short for a fastq file and "gz" indicates it was compressed.

```{r}
tic()

fns = list.files(path_trimmed)
fns = grep("_PT2.fq.gz",fns,value=TRUE)

### Load forward and reverse reads
fastqs <- fns[grepl(".fq.gz$", fns)]
fastqs <- sort(fastqs) # Sort ensures forward/reverse reads are in same order
fnFs <- fastqs[grepl("_1_PT2", fastqs)] # Just the forward read files
fnRs <- fastqs[grepl("_2_PT2", fastqs)] # Just the reverse read files
fnFs = file.path(path_trimmed, fnFs)
fnRs = file.path(path_trimmed, fnRs)

sample.names <- sapply(strsplit(basename(fnFs), "_1_PT2"), `[`, 1)
sample.names
toc()
```

# Step 5: Prepare to filter and trim the reads. 

Assign the filenames for the filtered fastq.gz files. The below step will create a sub-directory (a.k.a. a sub-folder) for the filtered files.

```{r}
tic()
filtFs <- file.path(path_trimmed, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path_trimmed, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
toc()
```

# Step 6: Filter and trim the reads

This step will perform quality assessments of each read:

1. `truncLen=c(245,230)`: Truncate reads based on length - 245 base pair for forward reads and 230 base pair for reverse reads,
2. `maxN=0`: Filter out reads that have any "N" (an unknown base), 
3. `maxEE=c(2,3)`: Allow up to two expected errors (defined below) in the forward reads and 3 in the reverse reads, 
4. `truncQ=1`: Truncate reads at the first instance of a quality score less than or equal to 1,
5. `rm.phix=TRUE`: Discard reads that map to the phiX genome.

Expected errors are calculated from the nominal definition of the quality score: 

$$
EE = sum(10^ {-Q/10} )
$$

These steps are performed in sequence (e.g. truncation occurs before filtering for "N"s). 

This `compress=TRUE` indicates to generate `.fasta.gz` files rather than `.fasta` files and `multithread=TRUE` enables multithreading of the function so it can run faster. However, on Windows machines, set `multithread=FALSE`.

```{r}
tic()
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(230,200), 
                     maxN=0, maxEE=c(2,3), truncQ=1, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE)
head(out)
toc()
```

All of these parameters (`truncLen`,`maxN`,`maxEE`, and `truncQ`) should be selected based on your dataset by evaluating the FastQC/MultiQC outputs for each sequence file. Alternatively, you can inspect quality of reads in R using the following code:

```{r}
plotQualityProfile(fnFs, aggregate = T)
```

More on FastQC (and MultiQC) in the lecture from Pankaj Kumar!

```{r}
plotQualityProfile(fnRs, aggregate = T)
```

# Step 7: Learn the Error Rates

The dada2 tutorial describes this step as follows:

> The DADA2 algorithm makes use of a parametric error model (`err`) and every amplicon dataset has a different set of error rates. The `learnErrors` method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

Basically, dada2 will look at a subset of reads, align them to identify the 'correct' call for each base pair in each consensus sequence, and build a model to learn how often a base will be miscalled. We can then use this model to infer true sequences in the next step.

```{r}
tic()
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
toc()
```

Plot these error rates as a function of the base pair quality score. Higher quality scores mean we have more confidence in the base pair, so we anticipate higher quality scores would generate a lower error rate.

```{r}

plotErrors(errF, nominalQ=TRUE)

```

Explanation from the dada2 tutorial:
>The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.

# Step 8: Sample Inference

Here, we will identify unique consensus sequences and their frequency from the filtered and trimmed sequence data. The error models will be used in this process!

Let's look at the forward reads first.

```{r}
tic()
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
toc()
```

Now let's look at the reverse reads.

```{r}

dadaRs <- dada(filtRs, err=errR, multithread=TRUE)

```

# Step 9: Merge paired reads 

We will now merge the forward and reverse reads together to obtain full sequences. Take a look at the figure below^[https://drive5.com/usearch/manual/merge_pair.html] to get a sense for how this is done.

![Merging paired end reads](/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/merging_reads.gif)

We will align error corrected, trimmed consensus sequences from the forward and reverse reads (called at this stage "denoised"). We must use the reverse-complement of the reverse reads. By dada2 default, merging will only occur if the forward and reverse reads overlap by at least 12 bases and are identical to each other in the overlap region (unlike the figure above). We can change these defaults though and, for this study, we'll set the maximum number of mismatches in the merged read to be 1 with `maxMismatch = 1`.

```{r}
tic()
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE, maxMismatch = 1)
toc()
```

Keep in mind that 1 mismatch in a 250 base pair read is a 0.4% difference in sequence. By convention, 97% similarity is often used to describe genus-level similarity. However, some pairs of genera have V4 regions with much more similarity than that! Default parameters are often chosen with great care, so deviate only if you have a good biological reason to do so!

# Step 10: Construct sequence table 

The resultant reads are called amplicon sequence variants (ASV), generally thought of as a higher-resolution version of the traditional Operatinal Taxonomic Unit (OTUs). Greg Medlock will talk more about ASVs v. OTUs! ASVs can be viewed as one group of microbes, frequently a genus, but variation among these microbes can occur!

Let's quantify each ASV in an ASV table, a higher-resolution version of the OTU table produced by traditional methods.

```{r}

seqtab <- makeSequenceTable(mergers)

```

Now we can plot the distribution of read lengths in this table:

```{r}
#  Inspect distribution of sequence lengths
plot(table(nchar(getSequences(seqtab))), xlab = "read length", ylab="number of reads")

```

We anticipated seeing most reads to be about 250bp, based on the experimental protocol. Too long of reads is a red flag that your adapter and/or primer trimming steps did not work. Note: we performed adapter and primer trimming before sharing the data with you. Check out the `trim_primers.sh` script for an example of these steps.

# Step 11: Remove chimeras 

The core dada2 method corrects for small errors (e.g. substitutions and indels), not chimeras. Chimeras are two sequences that are normally separated, that become joined by accident in the experimental steps of library construction.

Per the dada2 tutorial:
>Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
```

Let's again plot the read length distribution.

```{r}
plot(table(nchar(getSequences(seqtab.nochim))), xlab = "read length", ylab="number of reads")
```

# Step 12: Track reads through the pipeline 

Another great quality control step is to look at the number of reads at each step in the pipeline. We anticipate that each step will reduce the number of reads for each sample and that the biggest loss of reads will occur between the input sample and filtered samples. If you are losing a lot of reads later in the pipeline (especially the merging step!), your reads might not have been appropriately trimmed for adapters/primers or quality trimmed and filtered.

```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track
```

Write this QC table to a file for future reference.

```{r}
write.table(track, paste(path_results,"/Data-QC-Summary-Table.txt",sep = ''))
```

# Step 13: Assign taxonomy 

To make these results interpretable, we will assign taxonomy to the sequence variants. Essentially, we will map ASVs to known sequences from a database, stored in the file `silva_nr_v138_train_set.fa`. This process is similar to aligning sequences to a reference genome. This file `silva_nr_v138_train_set.fa` was obtained from the dada2 package but we have shared it directly with you to make things easier. More on this reference database and other options in the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html).

As summarized in the dada2 tutorial:

> The dada2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The `assignTaxonomy` function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least `minBoot` bootstrap confidence.

```{r}
tic()
taxa <- assignTaxonomy(seqtab.nochim, paste(path_reference,"/silva_nr_v138_train_set.fa.txt",sep=''), multithread=TRUE)
toc()
```

# Step 14: Assign taxonomy (now at the species level)

The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. For this and the first `assignTaxonomy` steps, if there is no nonambiguous solution, the ASV will not be mapped at this level. So if an ASV could map to multiple species within one genus, only the genus level identification will be provided. If an ASV could map to multiple genera, the genus will not be provided.

```{r}
tic()
taxa <- addSpecies(taxa, paste(path_reference,"/silva_species_assignment_v138.fa.txt",sep=''))
toc()
```

Let's examine the ASV assignments.

```{r}

taxa.print <- taxa #  Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)

```

# Step 14: Read in metadata file

```{r}

metadata = read.table(paste(path_results,"/TUMI_16S_metadata.csv",sep = ''), header=T, sep=",", row.names = 1)
metadata$Sample = rownames(metadata)
metadata

```

# Step 15: Construct Phyloseq object

We now construct a phyloseq object directly from the dada2 outputs. The benefit of using the package phyloseq is that sample metadata, ASV counts, and ASV taxonomy are tightly linked. If one sample is deleted, it's ASV counts are also removed. Here is a really useful diagram from [the documentation](https://bioconductor.riken.jp/packages/3.0/bioc/vignettes/phyloseq/inst/doc/phyloseq-basics.html) describing the structure.

![Phyloseq data structure](/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/phyloseq_obj_structure.png)

```{r}

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(metadata), 
               tax_table(taxa))

# first remove non-bacteria
og_taxa = data.frame(tax_table(ps))
keep = rownames(og_taxa[og_taxa$Kingdom %in% c("Bacteria","Archaea"),])
ps = prune_taxa(keep,ps)
keep = rownames(og_taxa[og_taxa$Family != "Mitochondria",])
ps = prune_taxa(keep,ps)
keep = rownames(og_taxa[og_taxa$Class != "Chloroplast",])
ps = prune_taxa(keep,ps)

```

# Step 16: Construct Phyloseq object

> It is more convenient to use short names for ASVs (e.g. ASV21) rather than the full DNA sequence when working with some of the tables and visualizations from phyloseq, but we want to keep the full DNA sequences for other purposes like merging with other datasets or indexing into reference databases like the Earth Microbiome Project. For that reason we’ll store the DNA sequences of our ASVs in the "refseq slot" of the phyloseq object, and then rename the taxa to a short string. That way, the short new taxa names will appear in tables and plots, and we can still recover the DNA sequences corresponding to each ASV as needed with refseq(ps).

Keep in mind ASV21 will be a different ASV in different studies!

```{r}

dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
```

# Step 17: Visualize diversity metrics

Diversity, a way to quantify a community structure, can be measured in multiple ways: alpha and beta. (And gamma diversity summarizes both!) Each of these features of a community can be calculated in multiple different ways.

Alpha diversity refers to how much variation there is in one sample. This is often described as "a local measure" and can be described as evenness and richness (see figure below^[https://ib.bioninja.com.au/options/option-c-ecology-and-conser/c4-conservation-of-biodiver/biodiversity.html]). To query the evenness of a sample, you would ask: are the existing ASVs even in abundance or do some dominate? Example calculations include Simpson's and Shannon's indeces.

![Richness and evenness](/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/diversity.jpg)
To query the richness of a sample, you would ask: how many ASVs are measured in this sample? This is the raw number of ASVs measured.

Beta diversity refers to diversity of a sample in comparison to others. Simply, if you were to perform ordination, how close would this sample be in relation to other samples? This is calculated through a variety of means including Jaccard distance or the phylogenetically-motivated UniFrac distance.

## Alpha diversity

Let's first calculate the alpha diversity with several metrics. Keep in mind, each metric tells you something a little different about the data and you do NOT want to just perform each until you get a signficant difference in your groups!

```{r}

plot_richness(ps, x="Diet", measures=c("Observed", "Chao1", "ACE", "Shannon", "Simpson", "InvSimpson", "Fisher"),) + geom_boxplot() + geom_point()

```

We generally see a decreased diversity in the mice fed a supplemented diet.

The question is, is this decreased diversity significant and meaningful? Let's first test if it is significant.

First, let's quantify the diversity.

```{r}
rich = estimate_richness(ps)
rich
```

We can use a non-parametric test, the Wilcoxon rank-sum test (Mann-Whitney) to test whether the Shannon index or observed number of OTUs differs significantly groups.

Shannon:

```{r}

pairwise.wilcox.test(rich$Shannon, p.adjust.method = "bonferroni", sample_data(ps)$Diet, exact = FALSE)

```


Observed number of ASVs:

```{r}

pairwise.wilcox.test(rich$Observed, p.adjust.method = "bonferroni", sample_data(ps)$Diet, exact = FALSE)

```

## Beta diversity

We will first perform ordination using Bray-Curtis distance. This distance metric is "a statistic used to quantify the compositional dissimilarity between two different sites."^[Wiki]

```{r}

#  Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")

```

Plot it:

```{r}

plot_ordination(ps.prop, ord.nmds.bray, color="Diet", label = "Sample", title="Bray NMDS")

```

You can see a separation between the sample groups, with the exception of one outlier-like mouse in each group.

We can also ordinate using a distance metric (UniFrac) that factors in phylogenetic relationships between detected ASVs, but we will first need to calculate phylogenetic distance.

```{r}

random_tree = ape::rtree(ntaxa(ps), rooted=TRUE, tip.label=taxa_names(ps))
samples.out = rownames(seqtab.nochim)
ps = merge_phyloseq(ps, samples.out, random_tree)

```

Now plot PCoA using unweighted UniFrac as distance

```{r}

wunifrac_dist = phyloseq::distance(ps, method="unifrac", weighted=F)
ordination = ordinate(ps.prop, method="PCoA", distance=wunifrac_dist)
plot_ordination(ps, ordination, color="Diet", label = "Sample") + theme(aspect.ratio=1)

```

We see better separation of sample groups now. What does this mean?

# Step 18: Plot observed differences in detected ASVs

```{r}
tic()
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Sample", fill="Genus") + facet_wrap(~Diet, scales="free_x")
toc()
```

What if we want to look at a higher phylogenetic level, like family?

```{r}
tic()
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Sample", fill="Family") + facet_wrap(~Diet, scales="free_x")
toc()
```
Is this what you'd expect to see based on the defined communities used to colonize the gnotobiotic mice?

# Step 19: Perform univariate statistics

Univariate statistics involve the comparison of one variable (hence UNIvariate) at a time across conditions. There are some key limitations to using univariate analyses for big biological data. First, univariate statistics rely on assumptions of indpendence. However, we know that there are relationships between microbes that would make their abundances dependent on one another. Additionally, host or diet-driven differences may influence a large group of microbes in the same way. So members of the microbiome (or metabolites or gene expression) are not independent.

Despite this, it's hard to get a 16S rRNA dataset published without discussing differentially abundant ASVs and providing a statistical basis for that conclusion. We will discuss other approaches (specifically, multivariate approaches) on Thursday for the sake of time.

If you are going to use univariate statistics, it is wise to use an approach that is robust to sparse data (that is, datasets that have many missing or nondetectable values). DESeq2 is one such approach.

Let's use it to identify the most differentially abundant microbes in the dataset.

First, however, we know mice were colonized with a defined community of 25 microbes, but we've identified `r ntaxa(ps)` ASVs. Let's investigate.

```{r}

asv_count = t(otu_table(ps))
summarized_asv_count = as.data.frame(rowSums(asv_count))
colnames(summarized_asv_count) = 'total_reads'
summarized_asv_count$ASV = rownames(summarized_asv_count)
ggplot(data = summarized_asv_count) + geom_histogram(aes(x = total_reads), bins = 100)

```

What if we only look at ASVs that have more than 100 reads across all samples. There are `r nrow(summarized_asv_count)` ASVs in this dataset. (better!)

```{r}

subset_table = summarized_asv_count[summarized_asv_count$total_reads>100,]
ggplot(data = subset_table) + geom_histogram(aes(x = total_reads), bins = 100)

```

Prep the data for univariate analysis using only the ASVs with over 100 reads.

```{r}

# transpose and replace zeros
asv_count = t(otu_table(ps))
asv_count = asv_count[rowSums(asv_count) > 100,]
# DESeq cannot handle non detected variables, so imput tiny number
asv_count[asv_count == 0] = min(asv_count)/10000

# prep the metadata
meta_use = metadata[,c('Diet','Sample')]
rownames(meta_use) = meta_use$Sample
# set order to be the same as in asv_count file
meta_use = meta_use[colnames(asv_count),]

```

Use DESeq2.

In brief, from the authors of DESeq2:

> DESeq2 provides methods to test for differential expression by use of negative binomial generalized linear models; the estimates of dispersion and logarithmic fold changes incorporate data-driven prior distributions. 

This package was built for RNASeq analysis, but can be used for other types of datasets like 16S. First, we will designate our samples, metadata, and count data.

```{r}
# make DESeq obj
dds = DESeqDataSetFromMatrix(countData = as.data.frame(asv_count),
                              colData = meta_use,
                              design= ~Diet) # design indicates our grouping variable(s)
```

Perform the analysis.

```{r}
dds_res = DESeq(dds)
results(dds_res)
```

The authors of DESeq2 say:

> Shrinkage of effect size (LFC estimates) is useful for visualization and ranking of genes.

Basically, lowly expressed genes tend to have higher variability and shrinking the log fold change (LFC) estimates make it easier to compare high and lowly expressed genes. We will use the default approach (`apeglm`).

```{r}
# because we are interested in treated vs untreated, we set 'coef=2'
resNorm <- lfcShrink(dds_res, coef=2, type="apeglm") # resultsNames(dds_proc)[2]

summary(resNorm)

```

Let's plot these results with a volcano plot.

```{r}

resNorm_df = as.data.frame(resNorm) 
resNorm_df$ASV = rownames(resNorm_df)
resNorm_df$col = ifelse(resNorm_df$padj > 0.05, 0,1)

# Label ASVs
tax = tax_table(ps)
tax = tax[rownames(tax) %in% resNorm_df$ASV,]
tax = merge(tax,resNorm_df, by.x= 'row.names', by.y = "ASV", all.x = F, all.y = T)
tax$Genus = as.character(tax$Genus)
tax2 = as.data.frame(tax)
tax2$id = rownames(tax2)
tax2$var = ifelse(is.na(tax2$Family),
                 as.character(tax2$id),
                 ifelse(is.na(tax2$Genus),paste0('[f] ',tax2$Family),tax2$Genus))

ggplot(tax2) + 
  geom_point(aes(x = log2FoldChange, y = log10(padj), color = as.factor(col)), alpha = .2)+
  xlab("Effect size: log2(fold change)") + ylab("-log10(adjusted p-value)") +
  #scale_color_manual(values = c('black','blue')) +
  scale_y_reverse() + guides(color = FALSE)

```
```{r}
ggplot(tax2) + 
  geom_point(aes(x = log2FoldChange, y = log10(padj), color = as.factor(col)), alpha = .2)+
  xlab("Effect size: log2(fold change)") + ylab("-log10(adjusted p-value)") +
  scale_color_manual(values = c('black','blue')) +
  scale_y_reverse() + guides(color = FALSE) +
  geom_label_repel(data = tax2[tax2$col == 1,],
                  aes(x = log2FoldChange, y = log10(padj), color = as.factor(col), label = var))

```

## Don't forget multiple testing correction!

# Next steps:

We can also use multivariate statistics and machine learning, however we will focus on those techniques on Thursday with the metabolomics data!
