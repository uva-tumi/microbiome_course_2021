---
title: "16S rRNA gene sequencing analysis"
author: "Pankaj Kumar, Maureen Carey"
output:
  html_document:
    df_print: paged
---

# Step 0: Get the data!

Sequencing data typically comes from a sequencing core or company demultiplexed as fastq files. Fastq files contain both the sequences and quality information about each base pair in the sequence. For this study, those data can be found here.

FYI the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html) is a great extra resource for you!

# Step 1: Install necessary packages

See installation_instructions file!

# Step 2: Load the computational environment

Note: you will need to change your root directory. This is where you installed the course folder.

```{r}
library("dada2")
library("tictoc")

library(phyloseq)
library(Biostrings)
library(ggplot2)

knitr::opts_knit$set(root.dir = '/Users/maureencarey/local_documents/work/TUMI_efforts/microbiome_course_2021/16S/data/sequence_data/trimmed')
```

Tictoc is a package that provides timing functions. Basically, if you run `tic()` BEFORE and `toc()` AFTER a set of commands, then you will see how long those commands took to run. Sequence analyses are generally quite slow, so we've included these functions for you to evaluate the speed of your computer. If you are struggling with slow running steps, just ask and we can share the results of intermediate steps with you! Running tic/toc is not necessary, but can be helpful!

# Step 3: Set directory path containing the fastq files

These fastq files were generated by 2x250 Illumina Miseq amplicon sequencing of the V4 region of the 16S rRNA gene (for more information about the data refer to the Tuesday lecture or check in with Carrie Cowardin). 

Please note you will need to change your path in the next code block to wherever you saved the data yesterday.

```{r, eval = F}
tic()
path <- paste("/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/data/sequence_data/trimmed")
list.files(path)
toc()
```

# Step 4: Read the names of the fastq files.

Perform some string manipulation to get matched lists of the forward and reverse fastq files.

Forward and reverse fastq filenames have format: 
* SAMPLENAME_1_PT2.fq.gz
* SAMPLENAME_2_PT2.fq.gz

```{r, eval = F}
tic()

fns = list.files(path)
fns = grep("_PT2.fq.gz",fns,value=TRUE)

### Load forward and reverse reads
fastqs <- fns[grepl(".fq.gz$", fns)]
fastqs <- sort(fastqs) # Sort ensures forward/reverse reads are in same order
fnFs <- fastqs[grepl("_1_PT2", fastqs)] # Just the forward read files
fnRs <- fastqs[grepl("_2_PT2", fastqs)] # Just the reverse read files
fnFs = file.path(path, fnFs)
fnRs = file.path(path, fnRs)

sample.names <- sapply(strsplit(basename(fnFs), "_1_PT2"), `[`, 1)
sample.names
toc()
```

# Step 5: Prepare to filter and trim the reads. 

Assign the filenames for the filtered fastq.gz files. The below step will creat a filtered sub-directory.

```{r, eval = F}
tic()
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
toc()
```

# Step 6: Filter and trim the reads

This step will perform quality assessments of each read:

1. `truncLen=c(245,230)`: Truncate reads based on length - 245 base pair for forward reads and 230 base pair for reverse reads,
2. `maxN=0`: Filter out reads that have any "N" (an unknown base), 
3. `maxEE=c(2,3)`: Allow up to two expected errors (defined below) in the forward reads and 3 in the reverse reads, 
4. `truncQ=1`: Truncate reads at the first instance of a quality score less than or equal to 1,
5. `rm.phix=TRUE`: Discard reads that map to the phiX genome.

Expected errors are calculated from the nominal definition of the quality score: 

$$
EE = sum(10^ (-Q/10) )
$$

These steps are performed in sequence (e.g. truncation occurs before filtering for "N"s). 

This `compress=TRUE` indicates to generate ".fasta.gz" files rather than ".fasta" files and `multithread=TRUE` enables multithreading of the function so it can run faster. However, on Windows machines, set `multithread=FALSE`.

```{r, eval = F}
tic()
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(220,200), 
                     maxN=0, maxEE=c(2,3), truncQ=1, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE)
head(out)
toc()
```

All of these parameters (`truncLen`,`maxN`,`maxEE`, and `truncQ`) should be selected based on your dataset by evaluating the FastQC outputs for each sequence file. Alternatively, you can inspect quality of reads in R using the following code:

```{r, eval = F}
plotQualityProfile(fnFs, aggregate = T)
```


```{r, eval = F}
plotQualityProfile(fnRs, aggregate = T)
```


# Step 7: Learn the Error Rates

The dada2 tutorial describes this step as follows:

> The DADA2 algorithm makes use of a parametric error model (`err`) and every amplicon dataset has a different set of error rates. The `learnErrors` method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

Basically, 

```{r, fig.width =5, fig.height=2, eval = F}
tic()
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
toc()
```

Plot these error rates.

```{r, fig.width =3, fig.height=2, eval = F}
tic()
plotErrors(errF, nominalQ=TRUE)
toc()
```

Explanation from the dada2 tutorial:
>The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. Here the estimated error rates (black line) are a good fit to the observed rates (points), and the error rates drop with increased quality as expected. Everything looks reasonable and we proceed with confidence.

# Step 8: Sample Inference

Here, we will identify unique sequence and their frequency from the filtered and trimmed sequence data.

Let's look at the forward reads first.

```{r, eval = F}
tic()
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
toc()
```

Now let's look at the reverse reads.

```{r, eval = F}
tic()
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
toc()
```

# Step 9: Merge paired reads 

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only retained if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region. 

Again, these parameters can be changed via function arguments. For example, here, we'll set the maximum number of mismatches in the merged read to be 1 with `maxMismatch = 1`.

```{r, eval = F}
tic()
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE, maxMismatch = 1)
toc()
```

Keep in mind that 1 mismatch in a 250 base pair read is a 0.4% difference in sequence. 97% similarity is often usedto describe genus-level similarity by convention. However, some pairs of genera have V4 regions with much more similarity than that!

# Step 10: Construct sequence table 

We can now construct an amplicon sequence variant table (ASV) table, a higher-resolution version of the OTU table produced by traditional methods.

```{r, eval = F}
tic()
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
toc()
```

Now we can plot it:

```{r, fig.width =4, fig.height=2, eval = F}
#  Inspect distribution of sequence lengths
plot(table(nchar(getSequences(seqtab))), xlab = "read length", ylab="number of reads")

```
We anticipated seeing most reads to be about 250bp, based on the experimental protocol. Too long of reads is a red flag that your adapter and/or primer trimming steps did not work.

# Step 11: Remove chimeras 

The core dada2 method corrects substitution and indel errors, but chimeras remain. Chimeras are two sequences that are normally separated, that become joined by accident in the experimental steps of library construction.

Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

```{r, eval = F}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
```

Let's plot it.

```{r, fig.width =4, fig.height=2, eval = F}
plot(table(nchar(getSequences(seqtab.nochim))), xlab = "read length", ylab="number of reads")
```

# Step 12: Track reads through the pipeline 

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r, eval = F}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))

colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
track
```

Write QC table to a file. Note: you will need to change the path here!

```{r, eval = F}
write.table(track, "./Data-QC-Summary-Table.txt")
```

```{r, eval = F}
samples.out <- rownames(seqtab.nochim)
write.table(samples.out, "./sample-meta-data-onlyrow")
```

# Step 13: Assign taxonomy 

To make these results interpretable, we will assign taxonomy to the sequence variants. Essentially, we will map ASVs to known sequences from a database, stored in the file `silva_nr_v138_train_set.fa`. This process is similar to aligning sequences to a reference genome. 
This file `silva_nr_v138_train_set.fa` was obtained from the dada2 package but we have shared it directly with you to make things easier. More on this reference database and others in the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html).

As summarized in the dada2 tutorial:

> The dada2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The `assignTaxonomy` function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least `minBoot` bootstrap confidence.

```{r, eval = F}
tic()
taxa <- assignTaxonomy(seqtab.nochim, "/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/silva_nr_v138_train_set.fa.txt", multithread=TRUE)
toc()
```

# Step 14: Assign taxonomy (now at the species level)

The dada2 package also implements a method to make species level assignments based on exact matching between ASVs and sequenced reference strains. For this and the first `assignTaxonomy` steps, if there is no nonambiguous solution, the ASV will not be mapped at this level. So if an ASV could map to multiple species within one genus, only the genus level identification will be provided. If an ASV could map to multiple genera, the genus will not be provided.

```{r, eval = F}
tic()
taxa <- addSpecies(taxa, "/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/silva_species_assignment_v138.fa.txt")
toc()
```

Let's examine the ASV assignments.

```{r, eval = F}
tic()
taxa.print <- taxa #  Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
toc()
```

# Step 14: Read in metadata file

```{r, eval = F}
tic()
samdf <- read.table("/Users/mac9jc/Documents/work/TUMI/microbiome_course_2021/16S/data/TUMI_16S_metadata.csv", header=F, sep=",", row.names = 1)
samdf
toc()
```

# Step 15: Construct Phyloseq object

We now construct a phyloseq object directly from the dada2 outputs. The benefit of using the package phyloseq is that sample metadata, ASV counts, and ASV taxonomy are tightly linked. If one sample is deleted, it's ASV counts are also removed. Here is a really useful diagram from [the documentation](https://bioconductor.riken.jp/packages/3.0/bioc/vignettes/phyloseq/inst/doc/phyloseq-basics.html) describing the structure.

!(Phyloseq data structure)[./16S/phyloseq_obj_structure.png]

```{r, eval = F}
tic()
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa))
toc()
```

# Step 16: Construct Phyloseq object

It is more convenient to use short names for ASVs (e.g. ASV21) rather than the full DNA sequence when working with some of the tables and visualizations from phyloseq, but we want to keep the full DNA sequences for other purposes like merging with other datasets or indexing into reference databases like the Earth Microbiome Project. For that reason we’ll store the DNA sequences of our ASVs in the "refseq slot" of the phyloseq object, and then rename the taxa to a short string. That way, the short new taxa names will appear in tables and plots, and we can still recover the DNA sequences corresponding to each ASV as needed with refseq(ps).


```{r, eval = F}
tic()
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
toc()
```


# Step 17: Exporting OTU table from phyloseq object
At this moment if you may wish you may export the OTU table


```{r, eval = F}
tic()
#  Extract abundance matrix from the phyloseq object
OTU1 = as(otu_table(ps), "matrix")
#  transpose if necessary
if(taxa_are_rows(ps)){OTU1 <- t(OTU1)}
#  Coerce to data.frame
OTUdf = as.data.frame(OTU1)
write.csv(OTUdf, "/Users/pk7z/Desktop/BIOINFORMATICS-CORE/TUMI-COURSE-WORK/TUMI_16S-OTU-Table.phyloseq.csv")
toc()
```

# Step 18: Visualize alpha-diversity:

```{r, fig.width =4, fig.height=2, eval = F}
tic()
plot_richness(ps, x="V12", measures=c("Observed", "Chao1", "ACE", "Shannon", "Simpson", "InvSimpson", "Fisher"),) 
toc()
```

# Comment: Obvious systematic difference in alpha-diversity between Control and Supplemented samples.

Same thing you may wish to plot as box-plot

```{r, fig.width =4, fig.height=2, eval = F}
tic()
plot_richness(ps, x="V12", measures=c("Observed", "Chao1", "ACE", "Shannon", "Simpson", "InvSimpson", "Fisher"),) + geom_boxplot()
toc()
```

# Richness estimate for calculating p-value

```{r, eval = F}
tic()
rich = estimate_richness(ps)
rich
toc()
```

# Step 19: Calculate if the difference between two groups of samples are significant or not

Test whether the observed number of OTUs differs significantly between Sample_Type. We make a non-parametric test, the Wilcoxon rank-sum test (Mann-Whitney):

Observed

```{r, eval = F}
tic()
pairwise.wilcox.test(rich$Observed, p.adjust.method = "bonferroni", sample_data(ps)$Diet, exact = FALSE)
toc()
```


Chao1

```{r, echo=FALSE, eval = F}
tic()
pairwise.wilcox.test(rich$Chao1, p.adjust.method = "bonferroni", sample_data(ps)$Diet, exact = FALSE)
toc()
```

You may calculate the p-value for other alpha diversity.



# Step 20: Bar plot:

```{r, fig.width =4, fig.height=2, eval = F}
tic()
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="SampleID", fill="Genus") + facet_wrap(~V12, scales="free_x")
toc()
```

# If you wish to make plot based on Family

```{r, fig.width =4, fig.height=2, eval = F}
tic()
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="SampleID", fill="Family") + facet_wrap(~Diet, scales="free_x")
toc()
```


# Step 21: Perform ordination

```{r, eval = F}
tic()
#  Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
toc()
```

Plot ordination

```{r, fig.width =4, fig.height=2, eval = F}
tic()
plot_ordination(ps.prop, ord.nmds.bray, color="Diet", label = "SampleID", title="Bray NMDS")
toc()
```

You can see a clear separation between the samples.

# Step 22: Calculate beta diversity

```{r, eval = F}
tic()
library("ape")
toc()
```

```{r, eval = F}
random_tree = rtree(ntaxa(ps), rooted=TRUE, tip.label=taxa_names(ps))
```

```{r, eval = F}
ps = merge_phyloseq(ps, samples.out, random_tree)
```

# Step 23: Plot the PCoA using the unweighted UniFrac as distance:

```{r, fig.width =3, fig.height=2, eval = F}
tic()
#  PCoA plot using the unweighted UniFrac as distance
wunifrac_dist = phyloseq::distance(ps, method="unifrac", weighted=F)
ordination = ordinate(ps.prop, method="PCoA", distance=wunifrac_dist)
plot_ordination(ps, ordination, color="Diet", label = "SampleID") + theme(aspect.ratio=1)
toc()
```

# Step 24: Plot Tree

```{r, fig.width =6, fig.height=9, eval = F}
plot_tree(ps, color="Diet", label.tips="Genus", size="abundance", plot.margin=0.2, text.size = 2)
```

# Step 25: Perform univariate statistics

Prep the data.

```{r}

# transpose and replace zeros
met_count = met_abund_normalized_to_blanks
met_count[met_count == 0] = min(met_count)/10000

# subset metadata for only Mock infected and PF infected samples 
meta_use = metadata[metadata$rawFileName %in% colnames(met_count),]

```

Use DESeq2.

```{r}
# make DESeq obj
dds_PD = DESeqDataSetFromMatrix(countData = as.data.frame(met_count),
                              colData = meta_use,
                              design= ~group)
dds_PD = DESeq(dds_PD)
# shrinkage
# because we are interested in treated vs untreated, we set 'coef=2'
resNorm_PD <- lfcShrink(dds_PD, coef=2, type="ashr") # resultsNames(dds_proc)[2]

#resOrdered <- resNorm_PD[order(resNorm_PD$pvalue),]
summary(resNorm_PD)

```

Make volcano plot.

```{r}

# Create the volcano plot - PREP FOR PREDETECTION
resNorm_PD_df = as.data.frame(resNorm_PD) %>% 
  rownames_to_column('ASV') %>%
  mutate(col = ifelse(padj<0.05,1,0),
         lab = ifelse(log2FoldChange>1.5 | abs(log2FoldChange) > 2,1,0),
         sig = -log10(padj))
sum(is.infinite(resNorm_PD_df$sig))
sum(is.na(resNorm_PD_df$sig))
resNorm_PD_df = resNorm_PD_df %>% mutate(padj = ifelse(is.na(padj),pvalue,padj),
                                         sig = ifelse(is.na(sig),0,-log10(pvalue)),
                                         col = ifelse(is.na(col),0,col),
                                         lab = ifelse(is.na(lab),0,lab))
#resNorm_PD[is.infinite(resNorm_PD$sig),"sig"] <- 350
sum(is.na(resNorm_PD_df$pvalue))
sum(is.infinite(resNorm_PD_df$pvalue))
#genes.to.plot <- !is.na(resNorm_PD$pvalue)

# Label ASVs
tax = tax_table(ps_rare_diff_temp_PD)
tax = tax[rownames(tax) %in% resNorm_PD_df$ASV,]
tax = merge(tax,resNorm_PD_df, by.x= 'row.names', by.y = "ASV", all.x = F, all.y = T)
tax$Genus = as.character(tax$Genus)
tax2 = tax %>% 
  transform(id=as.numeric(factor(Row.names))) %>%
  mutate(var = ifelse(is.na(Family),
                      as.character(Row.names),
                      ifelse(is.na(Genus),paste0('[f] ',Family),Genus)))

tax2 = merge(merge(tax2, as.data.frame(top10_PD)%>% rownames_to_column('Row.names'), by = "Row.names", all = T),
             as.data.frame(top10_TOD)%>% rownames_to_column('Row.names'), by = "Row.names", all = T)
tax2 = tax2 %>% 
  mutate(col2 = ifelse(col == 0, 'non sig',ifelse(!is.na(top10_TOD) & !is.na(top10_PD),'sig but dif color','sig')),
         size2 = ifelse(!is.na(top10_PD),1,0))


ggplot(predetection_tax2 %>% mutate(timepoint = 'pre-detection')) + 
  geom_point(aes(x = log2FoldChange, y = sig, color = as.factor(col2), size = as.factor(size2) ), alpha = .2)+
  xlab("Effect size: log2(fold change)") + ylab("-log10(adjusted p-value)")+
  scale_color_manual(values = c('black',"red","purple"),  guide = F) +
  scale_size_manual(values = c(1,3),  guide = F) + facet_wrap(~timepoint)
ggsave("/Users/maureencarey/local_documents/work/crypto_microbiome/cryptosporidium_microbiome/figures/Fig4B_univar_PD.png",
       width = 4,height = 4,units = "in",dpi = 300)

```

## Don't forget multiple testing correction!

